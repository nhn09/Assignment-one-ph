{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK_Basics.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNkneZQ5cd1WSoK3nwwYKLT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhn09/Assignment-one-ph/blob/main/NLTK_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "w-J3mYJ_-Yco"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdnZlUFHE1lD"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"Here, we're combining the popularity of board games like ludo and monopoly which have already sold over 250 million copies in the world, blending  with the power of gamification to help kids become more conscious. Most of us read social studies and memorized answers on \"climate justice,\" \"renewable sources,\" and \"green energy\" mainly because they were important for exams. Unfortunately, we were unaware of how significant these topics are in real-world situations; to be more particular, most of us fail to see how they are linked to our life, habits, and awareness.\"\"\"\n"
      ],
      "metadata": {
        "id": "USwcSb0lGZ0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "7hXLEZ2bGpHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  print(\"\\n\")\n",
        "\n",
        "words = nltk.word_tokenize(paragraph)\n",
        "len(words)\n",
        "for word in words:\n",
        "  print(word)"
      ],
      "metadata": {
        "id": "yspC2jSTG2JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming and Lammetization\n",
        "\n"
      ],
      "metadata": {
        "id": "rL-uSWy2IfYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "IL5OY7PVHzjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "P_XdgyGELdbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "ixi8b8j9NvuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " # stemming\n",
        "for i in range(len(sentences)):\n",
        "   words = nltk.word_tokenize(sentences[i])\n",
        "   words =  [stemmer.stem(word) for word in words if word not in set(stopwords.words ('english'))]\n",
        "   sentences[i] = ' '.join(words)\n",
        "   print(sentences[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uljlARGvIxi5",
        "outputId": "30d6d9ae-9bea-40ad-cd04-4d1ff00966aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here , 're combin popular board game like ludo monopoli alreadi sold 250 million copi world , blend power gamif help kid becom consciou .\n",
            "most u read social studi memor answer `` climat justic , '' `` renew sourc , '' `` green energi '' mainli import exam .\n",
            "unfortun , unawar signific topic real-world situat ; particular , u fail see link life , habit , awar .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYh-0NtTL8dK",
        "outputId": "28c82034-5015-4602-dcac-de0a05cd4c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"here , 're combin popular board game like ludo monopoli alreadi sold 250 million copi world , blend power gamif help kid becom consciou .\", \"most u read social studi memor answer `` climat justic , '' `` renew sourc , '' `` green energi '' mainli import exam .\", 'unfortun , unawar signific topic real-world situat ; particular , u fail see link life , habit , awar .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordNet Lammatizer"
      ],
      "metadata": {
        "id": "e7EqbK7IPyto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "abHQHk_LQOcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "FcSZ8heXPyJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "WpKa7UxLQHkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  lemwords= nltk.word_tokenize(sentences[i])\n",
        "  lemwords = [ lemmatizer.lemmatize(word) for word in lemwords if word not in set(stopwords.words('english'))]\n",
        "  sentences[i]= ' '.join(lemwords)\n",
        "  print( sentences[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjh31s1IQY-d",
        "outputId": "f453e8fc-2478-4f1f-b26e-e965dc02015e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here , 're combining popularity board game like ludo monopoly already sold 250 million copy world , blending power gamification help kid become conscious .\n",
            "Most u read social study memorized answer `` climate justice , '' `` renewable source , '' `` green energy '' mainly important exam .\n",
            "Unfortunately , unaware significant topic real-world situation ; particular , u fail see linked life , habit , awareness .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for each in sentences:\n",
        "  print(each)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfrefD57TiuS",
        "outputId": "14e284f5-bfb9-421b-ad23-f80e012b4955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here , 're combining popularity board game like ludo monopoly already sold 250 million copy world , blending power gamification help kid become conscious .\n",
            "Most u read social study memorized answer `` climate justice , '' `` renewable source , '' `` green energy '' mainly important exam .\n",
            "Unfortunately , unaware significant topic real-world situation ; particular , u fail see linked life , habit , awareness .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words"
      ],
      "metadata": {
        "id": "HSQhqY_L2gIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = \"\"\"Multi-word named entity recognition (NER)—the automated process of extracting from texts the names of people, places, and other lexical objects consisting of more than one word—is a core natural language processing (NLP) task. Current popular methods for NER used in Digital Humanities projects, such as those included in the Natural Language Toolkit or Stanford CoreNLP, use statistical models trained on sequential text, meaning that the context created by adjacent words in a text determines whether a word is tagged as an entity or as belonging to a named entity (NE). This short paper looks at a situation where statistical models cannot be used because sequential text is unavailable and suggests a “hack” for approximating multi-word NER under this constraint. Specifically, it looks at attempts to extract multi-word NEs in the HathiTrust Extracted Features (HTEF) dataset. The HTEF dataset provides page-level token counts for nearly 16 million volumes in the HathiTrust collection as an effort to provide “non- consumptive” access to book contents. That is, HTEF data is provided in a pseudo-random manner—a scrambled list of token counts—and not as words in a consecutive, readable word order. Accordingly, statistical NER methods cannot be used with HTEF data.\n",
        "\n",
        "In recent projects involving HTEF data, I have had initial success with extracting multi-word NEs with a different approach, namely by 1. creating permutations of page-level tokens provided in the HTEF that are likely to be NE constituents; and 2. querying these permutations against an open knowledge base, specifically Wikidata, in order to determine if they are valid NEs. The method, implemented in Python, can be summarized with the following example: 1. given the phrase “New York and New Jersey”, we construct a dictionary with the following word counts— {‘and’: 1, ‘Jersey’: 1, ‘New’: 2, ‘York’: 1}; 2. taking all of the permutations of potential NE constituents, here defined by capital letters, we construct the following list—[’Jersey New’, ‘Jersey York’, ‘New Jersey, ‘New York’, ‘York Jersey’, ‘York New’]; and, lastly, 3. querying Wikidata for all items in this list, we return only positive matches as valid entities, i.e. the following list—[‘New Jersey’, ‘New York’]. This method does not account for all possible multi-word NEs (for example, because of the capitalization constraint in step 2, “City of New York” would not be considered due to the lowercase ‘of’), but nevertheless represents a novel solution for performing a core NLP task on a pseudo-random text collection. Moreover, it represents a good example of using a general knowledge base, like Wikidata, as a validation mechanism in NLP tasks. Lastly, it represents an attempt to push the boundaries of what can be derived from the HTEF dataset, while also respecting its non-consumptive nature, i.e. it is only trying to extract information from the dataset based on the likelihood of adjacent tokens, not to reconstruct entire sentences, paragraphs, or pages. With these points considered, the paper should be a useful model for other text-focused Digital Humanities projects looking to extend NLP methods to non-consumptive text collections or similarly challenging text-as-data datasets.\"\"\""
      ],
      "metadata": {
        "id": "aVN-xdgX5d4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning the text\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "wordnet = WordNetLemmatizer()\n",
        "sentences = nltk.sent_tokenize(dummy)\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  review = re.sub('[^a-zA-Z]',' ', sentences[i]) #^ = not\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english')) ]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)\n"
      ],
      "metadata": {
        "id": "1BbK0GIq2flU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk2GVH3F-so4",
        "outputId": "9fedb009-34e5-488d-d0d2-2092ea61de47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['multi word named entity recognition ner automated process extracting text name people place lexical object consisting one word core natural language processing nlp task', 'current popular method ner used digital humanity project included natural language toolkit stanford corenlp use statistical model trained sequential text meaning context created adjacent word text determines whether word tagged entity belonging named entity ne', 'short paper look situation statistical model cannot used sequential text unavailable suggests hack approximating multi word ner constraint', 'specifically look attempt extract multi word ne hathitrust extracted feature htef dataset', 'htef dataset provides page level token count nearly million volume hathitrust collection effort provide non consumptive access book content', 'htef data provided pseudo random manner scrambled list token count word consecutive readable word order', 'accordingly statistical ner method cannot used htef data', 'recent project involving htef data initial success extracting multi word ne different approach namely creating permutation page level token provided htef likely ne constituent querying permutation open knowledge base specifically wikidata order determine valid ne', 'method implemented python summarized following example given phrase new york new jersey construct dictionary following word count jersey new york taking permutation potential ne constituent defined capital letter construct following list jersey new jersey york new jersey new york york jersey york new lastly querying wikidata item list return positive match valid entity e', 'following list new jersey new york', 'method account possible multi word ne example capitalization constraint step city new york would considered due lowercase nevertheless represents novel solution performing core nlp task pseudo random text collection', 'moreover represents good example using general knowledge base like wikidata validation mechanism nlp task', 'lastly represents attempt push boundary derived htef dataset also respecting non consumptive nature e', 'trying extract information dataset based likelihood adjacent token reconstruct entire sentence paragraph page', 'point considered paper useful model text focused digital humanity project looking extend nlp method non consumptive text collection similarly challenging text data datasets']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "Sy5Gb4si-2lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[1:2])"
      ],
      "metadata": {
        "id": "DslA7A4iDvB4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}